{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJ0ZKrDxffoG"
   },
   "source": [
    "# **Importing Necessary libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7iXDJSIGjjn-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 04:19:37.120619: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-24 04:19:37.120680: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-24 04:19:37.121770: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-24 04:19:37.128828: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import re # regular expression\n",
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzRVuD3qkGvD"
   },
   "source": [
    "# **Preparing the data named IMDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1681,
     "status": "ok",
     "timestamp": 1684231839549,
     "user": {
      "displayName": "KARNATI MOHAN",
      "userId": "16604712269571437600"
     },
     "user_tz": -330
    },
    "id": "RI03JfP7kEKA",
    "outputId": "7d9cfb0e-5ac8-4c11-ce7f-5fd56b2a28cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/workspace/Usrf/Day7/IMDB Dataset (2).csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4KcdMAKtQan"
   },
   "source": [
    "Stop Word is a commonly used words in a sentence, usually a search engine is programmed to ignore this words (i.e. \"the\", \"a\", \"an\", \"of\", etc.)\n",
    "Declaring the english stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1684231839549,
     "user": {
      "displayName": "KARNATI MOHAN",
      "userId": "16604712269571437600"
     },
     "user_tz": -330
    },
    "id": "xjJrIijipG5D",
    "outputId": "ebbb2feb-153b-4600-daf4-bc69a2120043"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlWrgDKttZC1"
   },
   "source": [
    "# **Load and Clean Dataset**\n",
    "**In the original dataset, the reviews are still dirty. There are still html tags, numbers, uppercase, and punctuations. This will not be good for training, so in load_dataset() function, beside loading the dataset using pandas, I also pre-process the reviews by removing html tags, non alphabet (punctuations and numbers), stop words, and lower case all of the reviews.**\n",
    "\n",
    "# **Encode Sentiments**\n",
    "**In the same function, We also encode the sentiments into integers (0 and 1). Where 0 is for negative sentiments and 1 is for positive sentiments.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8335,
     "status": "ok",
     "timestamp": 1684231847882,
     "user": {
      "displayName": "KARNATI MOHAN",
      "userId": "16604712269571437600"
     },
     "user_tz": -330
    },
    "id": "NM_5RjUNqa-s",
    "outputId": "30d7113d-cc6c-4202-b18c-97da68053e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews\n",
      "0        [one, reviewers, mentioned, watching, oz, epis...\n",
      "1        [a, wonderful, little, production, the, filmin...\n",
      "2        [i, thought, wonderful, way, spend, time, hot,...\n",
      "3        [basically, family, little, boy, jake, thinks,...\n",
      "4        [petter, mattei, love, time, money, visually, ...\n",
      "                               ...                        \n",
      "49995    [i, thought, movie, right, good, job, it, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [i, catholic, taught, parochial, elementary, s...\n",
      "49998    [i, going, disagree, previous, comment, side, ...\n",
      "49999    [no, one, expects, star, trek, movies, high, a...\n",
      "Name: review, Length: 50000, dtype: object \n",
      "\n",
      "Sentiment\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "49995    1\n",
      "49996    0\n",
      "49997    0\n",
      "49998    0\n",
      "49999    0\n",
      "Name: sentiment, Length: 50000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    df = pd.read_csv('/workspace/Usrf/Day7/IMDB Dataset (2).csv')\n",
    "    x_data = df['review']       # Reviews/Input\n",
    "    y_data = df['sentiment']    # Sentiment/Output\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "\n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "    y_data = y_data.replace('positive', 1)\n",
    "    y_data = y_data.replace('negative', 0)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = load_dataset()\n",
    "\n",
    "print('Reviews')\n",
    "print(x_data, '\\n')\n",
    "print('Sentiment')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcQbFcH0thD1"
   },
   "source": [
    "# **Split Dataset**\n",
    "**In this work, We decided to split the data into 80% of Training and 20% of Testing set using train_test_split method from Scikit-Learn. By using this method, it automatically shuffles the dataset. We need to shuffle the data because in the original dataset, the reviews and sentiments are in order, where they list positive reviews first and then negative reviews. By shuffling the data, it will be distributed equally in the model, so it will be more accurate for predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1684231847883,
     "user": {
      "displayName": "KARNATI MOHAN",
      "userId": "16604712269571437600"
     },
     "user_tz": -330
    },
    "id": "8cy2sU3jthtz",
    "outputId": "14a08333-0159-4fa5-a751-e51a990df70d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "32906    [this, essentially, variation, house, of, wax,...\n",
      "43969    [i, saw, hot, millions, tcm, i, completely, fo...\n",
      "16045    [ok, first, said, i, wanted, check, whether, m...\n",
      "30207    [i, saw, version, decade, ago, looking, ever, ...\n",
      "26341    [gordon, goes, top, typical, full, moon, fashi...\n",
      "                               ...                        \n",
      "26429    [i, believe, show, still, rating, i, could, se...\n",
      "15083    [yes, movie, hilarious, acting, top, notch, wh...\n",
      "5917     [i, avid, fan, violent, exploitation, cinema, ...\n",
      "30662    [make, wonder, first, place, this, movie, bad,...\n",
      "2063     [but, i, doubt, many, running, see, movie, or,...\n",
      "Name: review, Length: 40000, dtype: object \n",
      "\n",
      "48639    [when, i, kid, always, used, babysat, always, ...\n",
      "38906    [a, brilliant, movie, family, guilt, sacrifice...\n",
      "17539    [this, movie, terrible, throughout, whole, mov...\n",
      "48837    [i, enjoyed, carax, les, amants, du, pont, neu...\n",
      "23205    [stanley, iris, heart, warming, film, two, peo...\n",
      "                               ...                        \n",
      "37415    [in, desperate, thoroughly, silly, attempt, ke...\n",
      "13480    [if, directors, producers, publicists, wish, p...\n",
      "1811     [this, great, small, film, i, say, small, hund...\n",
      "32468    [with, rerelease, adv, films, i, chance, watch...\n",
      "13481    [charles, mcdougall, resume, includes, directi...\n",
      "Name: review, Length: 10000, dtype: object \n",
      "\n",
      "Test Set\n",
      "32906    1\n",
      "43969    1\n",
      "16045    0\n",
      "30207    1\n",
      "26341    0\n",
      "        ..\n",
      "26429    0\n",
      "15083    1\n",
      "5917     0\n",
      "30662    0\n",
      "2063     0\n",
      "Name: sentiment, Length: 40000, dtype: int64 \n",
      "\n",
      "48639    0\n",
      "38906    1\n",
      "17539    0\n",
      "48837    0\n",
      "23205    1\n",
      "        ..\n",
      "37415    0\n",
      "13480    0\n",
      "1811     1\n",
      "32468    1\n",
      "13481    1\n",
      "Name: sentiment, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "\n",
    "print('Train Set')\n",
    "print(x_train, '\\n')\n",
    "print(x_test, '\\n')\n",
    "print('Test Set')\n",
    "print(y_train, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B712-ID4tnDY"
   },
   "source": [
    "**Function for getting the average review length, by calculating the mean of all the reviews length (using numpy.mean)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5n86Ria5toOI"
   },
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwYgTbAEts_G"
   },
   "source": [
    "# **Tokenize and Pad/Truncate Reviews**\n",
    "**A Neural Network only accepts numeric data, so we need to encode the reviews. I use tensorflow.keras.preprocessing.text.Tokenizer to encode the reviews into integers, where each unique word is automatically indexed (using fit_on_texts method) based on x_train.**\n",
    "\n",
    "**x_train and x_test is converted into integers using texts_to_sequences method.**\n",
    "\n",
    "**Each reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length) using tensorflow.keras.preprocessing.sequence.pad_sequences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5100,
     "status": "ok",
     "timestamp": 1684231852973,
     "user": {
      "displayName": "KARNATI MOHAN",
      "userId": "16604712269571437600"
     },
     "user_tz": -330
    },
    "id": "_cRXNvFHttoe",
    "outputId": "70e45442-4ac0-4fc8-8971-808224a57fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words: 92597\n",
      "Encoded X Train\n",
      " [[    8  1852  6826 ...   356   259   516]\n",
      " [    1   121   777 ...     0     0     0]\n",
      " [  485    23   208 ... 14648  2894  1424]\n",
      " ...\n",
      " [    1  6302   233 ...   232  6876   134]\n",
      " [   25   486    23 ...     0     0     0]\n",
      " [   30     1   693 ...   150  3349  3349]] \n",
      "\n",
      "Encoded X Test\n",
      " [[  173     1   444 ...     0     0     0]\n",
      " [   39   419     3 ...     0     0     0]\n",
      " [    8     3   286 ...     0     0     0]\n",
      " ...\n",
      " [    8    21   310 ...     0     0     0]\n",
      " [  424 45002 40587 ...     0     0     0]\n",
      " [ 1135  7095  1736 ...   257  1254  1328]] \n",
      "\n",
      "Maximum review length:  130\n"
     ]
    }
   ],
   "source": [
    "# ENCODE REVIEW\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "print('Total Words:', total_words)\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXf1daJctzRK"
   },
   "source": [
    "# **Build Architecture/Model**\n",
    "**Embedding Layer: in simple terms, it creates word vectors of each word in the word_index and group words that are related or have similar meaning by analyzing other words around them.**\n",
    "\n",
    "**RNN Layer: to make a decision to keep or throw away data by considering the current input, previous output.**\n",
    "\n",
    "**Dense Layer: compute the input with the weight matrix and bias (optional), and using an activation function. I use Sigmoid activation function for this work because the output is only 0 or 1.**\n",
    "\n",
    "**The optimizer is Adam and the loss function is Binary Crossentropy because again the output is only 0 and 1, which is a binary number.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4478,
     "status": "ok",
     "timestamp": 1684231857441,
     "user": {
      "displayName": "KARNATI MOHAN",
      "userId": "16604712269571437600"
     },
     "user_tz": -330
    },
    "id": "FoM7MNAKtz50",
    "outputId": "1971c951-a587-4f37-9632-481b5fc6a896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 130, 32)           2963104   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 64)                6208      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2969377 (11.33 MB)\n",
      "Trainable params: 2969377 (11.33 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 04:19:48.446850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1926] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 17947 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:b7:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "rnn = Sequential()\n",
    "\n",
    "rnn.add(Embedding(total_words,32,input_length =max_length))\n",
    "rnn.add(SimpleRNN(64,input_shape = (total_words, max_length), return_sequences=False,activation=\"relu\"))\n",
    "rnn.add(Dense(1, activation = 'sigmoid')) #flatten\n",
    "\n",
    "print(rnn.summary())\n",
    "rnn.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqi0Y67xFeyZ"
   },
   "source": [
    "#**Trainin the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1043227,
     "status": "ok",
     "timestamp": 1684232900661,
     "user": {
      "displayName": "KARNATI MOHAN",
      "userId": "16604712269571437600"
     },
     "user_tz": -330
    },
    "id": "udaR9DEFu6Fr",
    "outputId": "2d7f4c98-14af-4fd1-d108-3457ab8e859b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 04:19:50.171122: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f4e3e3ba240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-24 04:19:50.171182: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB MIG 3g.20gb, Compute Capability 8.0\n",
      "2024-06-24 04:19:50.177000: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-06-24 04:19:50.213467: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:467] Loaded cuDNN version 90100\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1719202790.305639  519961 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 59s 183ms/step - loss: 0.6942 - accuracy: 0.5079\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 40s 129ms/step - loss: 0.7098 - accuracy: 0.5480\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 0.6572 - accuracy: 0.6007\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 28s 89ms/step - loss: 0.5317 - accuracy: 0.7504\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 26s 84ms/step - loss: 0.6123 - accuracy: 0.6037\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 25s 80ms/step - loss: 0.5757 - accuracy: 0.6673\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.5642 - accuracy: 0.6904\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.4223 - accuracy: 0.8372\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 0.3467 - accuracy: 0.8749\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.6464 - accuracy: 0.7937\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 22s 72ms/step - loss: 0.3721 - accuracy: 0.8348\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 0.2627 - accuracy: 0.9097\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 0.2142 - accuracy: 0.9275\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 23s 72ms/step - loss: 0.1729 - accuracy: 0.9419\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 0.1602 - accuracy: 0.9510\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 0.1506 - accuracy: 0.9554\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.1694 - accuracy: 0.9479\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 21s 66ms/step - loss: 0.1864 - accuracy: 0.9450\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 0.6754 - accuracy: 0.5836\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 0.5416 - accuracy: 0.6392\n"
     ]
    }
   ],
   "source": [
    "history = rnn.fit(x_train,y_train,epochs = 20,batch_size=128,verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h8KUOGpFhz2"
   },
   "source": [
    "# **Saving The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PGQ5CGWd7pLX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model = rnn.save('rnn.h5')\n",
    "loaded_model = load_model('rnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fKgbBP4Flqd"
   },
   "source": [
    "#**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1627,
     "status": "ok",
     "timestamp": 1684232902285,
     "user": {
      "displayName": "KARNATI MOHAN",
      "userId": "16604712269571437600"
     },
     "user_tz": -330
    },
    "id": "t6nkcsHsvMyI",
    "outputId": "5e99d090-0638-4a7a-b8b9-6ac25aa93132"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 10ms/step\n",
      "[[0.48266408]\n",
      " [0.48266298]\n",
      " [0.48266408]\n",
      " ...\n",
      " [0.4826619 ]\n",
      " [0.4826619 ]\n",
      " [0.56367934]]\n",
      "48639    0\n",
      "38906    1\n",
      "17539    0\n",
      "48837    0\n",
      "23205    1\n",
      "        ..\n",
      "37415    0\n",
      "13480    0\n",
      "1811     1\n",
      "32468    1\n",
      "13481    1\n",
      "Name: sentiment, Length: 10000, dtype: int64\n",
      "Correct Prediction: 5651\n",
      "Wrong Prediction: 4349\n",
      "Accuracy: 56.510000000000005\n"
     ]
    }
   ],
   "source": [
    "y_pred = rnn.predict(x_test, batch_size = 128)\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "for i in range(len(y_pred)):\n",
    "  if y_pred[i]>0.5:\n",
    "    y_pred[i] = 1\n",
    "  else:\n",
    "    y_pred[i] = 0\n",
    "\n",
    "true = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_pred[i]:\n",
    "        true += 1\n",
    "\n",
    "print('Correct Prediction: {}'.format(true))\n",
    "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
    "print('Accuracy: {}'.format(true/len(y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xb1KdQ_pNJIW"
   },
   "source": [
    "Message: **Nothing was typical about this. Everything was beautifully done in this movie, the story, the flow, the scenario, everything. I highly recommend it for mystery lovers, for anyone who wants to watch a good movie!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yN_6mY24Jlw"
   },
   "source": [
    "# **Example review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3796,
     "status": "ok",
     "timestamp": 1684232906077,
     "user": {
      "displayName": "KARNATI MOHAN",
      "userId": "16604712269571437600"
     },
     "user_tz": -330
    },
    "id": "1_lSReZWvapL",
    "outputId": "421e0566-a8fb-4bf7-fc8e-9332436c8bb9"
   },
   "outputs": [],
   "source": [
    "review = str(input('Movie Review: '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VTkE6KF4P31"
   },
   "source": [
    "#**Pre-processing of entered review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1684232906077,
     "user": {
      "displayName": "KARNATI MOHAN",
      "userId": "16604712269571437600"
     },
     "user_tz": -330
    },
    "id": "S-isbUeevaq5",
    "outputId": "3bf01824-0d16-41cd-b37c-03b17417e6bd"
   },
   "outputs": [],
   "source": [
    "# Pre-process input\n",
    "regex = re.compile(r'[^a-zA-Z\\s]') #Compile a regular expression to remove unwanted characters\n",
    "review = regex.sub('', review) #Remove unwanted characters from the review\n",
    "print('Cleaned: ', review)\n",
    "\n",
    "words = review.split(' ')\n",
    "filtered = [w for w in words if w not in english_stops] #Filter out stop words\n",
    "filtered = ' '.join(filtered) #Join the filtered words back into a string\n",
    "filtered = [filtered.lower()] #Convert the filtered string to lowercase\n",
    "\n",
    "print('Filtered: ', filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1684232906077,
     "user": {
      "displayName": "KARNATI MOHAN",
      "userId": "16604712269571437600"
     },
     "user_tz": -330
    },
    "id": "Te3OyQohvaua",
    "outputId": "6375cb0c-9cf0-4ff0-9d39-b497f0708bce"
   },
   "outputs": [],
   "source": [
    "tokenize_words = token.texts_to_sequences(filtered) #Tokenize the filtered text\n",
    "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "print(tokenize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9X_Haz8g4U7Q"
   },
   "source": [
    "#**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1684232906077,
     "user": {
      "displayName": "KARNATI MOHAN",
      "userId": "16604712269571437600"
     },
     "user_tz": -330
    },
    "id": "vVWAZXuZviIp",
    "outputId": "a721b9ae-3347-468a-ccd1-d9c1c6bf6619"
   },
   "outputs": [],
   "source": [
    "result = rnn.predict(tokenize_words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1684232906078,
     "user": {
      "displayName": "KARNATI MOHAN",
      "userId": "16604712269571437600"
     },
     "user_tz": -330
    },
    "id": "gVgQvGXBviME",
    "outputId": "c1e1e527-cd1e-4a41-9e6b-e0fabb1e9b10"
   },
   "outputs": [],
   "source": [
    "if result >= 0.7:\n",
    "    print('positive')\n",
    "else:\n",
    "    print('negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcagsy1Q8LUz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
